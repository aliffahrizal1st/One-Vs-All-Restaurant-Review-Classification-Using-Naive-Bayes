{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titipan Natha (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rFqf3_8vWrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c36e04-ca2d-45b6-ed1b-ef6b92377a16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "from copy import deepcopy\n",
        "from itertools import chain\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from google.colab import files, drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parsing_xml(data):\n",
        "  root = ET.parse(data).getroot()\n",
        "  list_ulasan,list_opini = [], []\n",
        "\n",
        "  for review in root.findall('Review'):\n",
        "    opini_review,ulasan_review = [], []\n",
        "    for i in review.findall('./sentences/sentence'):\n",
        "      teks = i.find('text').text\n",
        "      opinions = i.find('./Opinions/Opinion')\n",
        "      if teks is None or opinions is None:\n",
        "        continue\n",
        "      ulasan_review.append(\"\".join(teks))\n",
        "      opini_review.append({opinions.get('category'): opinions.get('polarity')})\n",
        "    list_ulasan.append(ulasan_review)\n",
        "    list_opini.append(opini_review)\n",
        "  return list_ulasan, list_opini"
      ],
      "metadata": {
        "id": "GtoR0hFEBIDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "  cleaned = []\n",
        "  for i in range(len(data)):\n",
        "    cleaned.append([])\n",
        "    for j in data[i]:\n",
        "      temp = (re.sub(r\"[^a-zA-Z\\s]+\", \"\", j)).lower()\n",
        "      temp = re.sub(r\" +\", \" \", temp)\n",
        "      cleaned[i].append(temp)\n",
        "  return cleaned"
      ],
      "metadata": {
        "id": "3XHdBnD13IDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(ulasan):\n",
        "  for i in ulasan:\n",
        "    for j in i:\n",
        "      j = re.sub(r\"won't\", \"will not\", j)\n",
        "      j = re.sub(r\"can\\'t\", \"can not\", j)\n",
        "      j = re.sub(r\"n\\'t\", \" not\", j)\n",
        "      j = re.sub(r\"\\'re\", \" are\", j)\n",
        "      j = re.sub(r\"\\'s\", \" is\", j)\n",
        "      j = re.sub(r\"\\'d\", \" would\", j)\n",
        "      j = re.sub(r\"\\'ll\", \" will\", j)\n",
        "      j = re.sub(r\"\\'t\", \" not\", j)\n",
        "      j = re.sub(r\"\\'ve\", \" have\", j)\n",
        "      j = re.sub(r\"\\'m\", \" am\", j)\n",
        "  return ulasan"
      ],
      "metadata": {
        "id": "R4Rruglw3gTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postag(review):\n",
        "  for i in review:\n",
        "    for j in range(len(i)):\n",
        "      i[j] = nltk.pos_tag(nltk.word_tokenize(i[j]))\n",
        "  return review"
      ],
      "metadata": {
        "id": "aW0dy3WANiHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opini_rule(result_postag):\n",
        "  grammar = \"NP: {<DT|PP|CD|RB>?<JJ|JJR|JJS>*<NN|NNS|PRP|NNP|VB|IN|PRP\\$>+<VBD|VBZ|VBN|VBP|VB|IN>*<JJ|JJS|RB>*<PRP|NN|NNS>*}\"\n",
        "  cp = nltk.RegexpParser(grammar)\n",
        "  for i in result_postag:\n",
        "    for j in range(len(i)):\n",
        "      i[j] = cp.parse(i[j])\n",
        "  return result_postag"
      ],
      "metadata": {
        "id": "n7sYcMLuOVrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opini_extractor(result_rule):\n",
        "  finish = []\n",
        "  for result in range(len(result_rule)):\n",
        "    temp = []\n",
        "    finish.append([])\n",
        "    for res in range(len(result_rule[result])):\n",
        "      temp.append([])\n",
        "      if type(result_rule[result][res]) == nltk.tree.Tree:\n",
        "        for restu in result_rule[result][res]:\n",
        "          if type(restu) == nltk.tree.Tree:\n",
        "            for rest in restu:\n",
        "              temp[res].append(rest[0])\n",
        "          else:\n",
        "            temp[res].append(restu[0])\n",
        "      if len(temp[res]) >= 2:\n",
        "        finish[result].append(\" \".join(temp[res]))\n",
        "  return finish"
      ],
      "metadata": {
        "id": "5YCV9T1P5ICD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_in_list_of_list(mylist, char):\n",
        "    for sub_list in mylist:\n",
        "        if char in sub_list:\n",
        "            return mylist.index(sub_list), sub_list.index(char)\n",
        "    raise ValueError(\"'{char}' is not in list\".format(char = char))"
      ],
      "metadata": {
        "id": "KHxUFbqrha73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def class_extractor(list_ulasan_old, list_ulasan_new, list_opini):\n",
        "  diff = []\n",
        "  for i in range(len(list_ulasan_old)):\n",
        "    for j in range(len(list_ulasan_old[i])):\n",
        "      if list_ulasan_old[i][j] not in list_ulasan_new[i]:\n",
        "        diff.append(list_ulasan_old[i][j])\n",
        "\n",
        "  for i in diff:\n",
        "    indA,indB = find_in_list_of_list(list_ulasan_old, i)\n",
        "    del list_opini[indA][indB]\n",
        "  return list_opini"
      ],
      "metadata": {
        "id": "3Cb5OeY8z1UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractFeature(text,results=[]):\n",
        "  for i in text:\n",
        "    for j in i:\n",
        "      for k in j.split():\n",
        "        if k not in results:\n",
        "          results.append(k)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "pp6WZ1NX8cZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def termFrequency(text,feature):\n",
        "  results = []\n",
        "  memory = []\n",
        "\n",
        "  for i in range(len(text)):\n",
        "    memory.append([])\n",
        "    for j in text[i]:\n",
        "      memory[i].append(j.split())\n",
        "  for a in range(len(memory)):\n",
        "    results.append([])\n",
        "    for b in range(len(memory[a])):\n",
        "      results[a].append([])\n",
        "      results[a][b] = [memory[a][b].count(feature[i]) for i in range(len(feature))]\n",
        "  \n",
        "  return results"
      ],
      "metadata": {
        "id": "IQpJqY0kBa-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def devideClass(class_ListTest):\n",
        "  aspectClass, sentimentClass = [], []\n",
        "  for i in class_ListTest:\n",
        "    for key, val in i.items():\n",
        "      aspectClass.append(key)\n",
        "      sentimentClass.append(val)\n",
        "  return aspectClass, sentimentClass"
      ],
      "metadata": {
        "id": "8KlKLsPmIR5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prior(docVal,docTarget, seekTarget=\"\"):\n",
        "  result = []\n",
        "  totalLengthDoc = len(docVal)\n",
        "  countTarget, countOther = 0,0\n",
        "  for i in docTarget:\n",
        "    for key, val in i.items():\n",
        "      if key == seekTarget:\n",
        "        countTarget+=1\n",
        "      else:\n",
        "        countOther+=1\n",
        "  result.append(countTarget/totalLengthDoc)\n",
        "  result.append(countOther/totalLengthDoc)\n",
        "  return result"
      ],
      "metadata": {
        "id": "narHeo9VAopX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordCounter(tfDocTrain, docTrainTarget, seekTarget=\"\"):\n",
        "  allWords = [[],[]]\n",
        "  for j in range(len(tfDocTrain[0])):\n",
        "    countWordsMain = 0\n",
        "    countWordsOther = 0\n",
        "    for i in range(len(tfDocTrain)):\n",
        "      for key, val in docTrainTarget[i].items():\n",
        "        if key == seekTarget:\n",
        "          countWordsMain+=tfDocTrain[i][j]\n",
        "        else:\n",
        "          countWordsOther+=tfDocTrain[i][j]\n",
        "    allWords[0].append(countWordsMain)\n",
        "    allWords[1].append(countWordsOther)\n",
        "  \n",
        "  return allWords"
      ],
      "metadata": {
        "id": "b2H1vvEiKCBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def likelihood(wordCounterVal, list_feature):\n",
        "  perClass = [[],[]]\n",
        "  for i in range(len(wordCounterVal)):\n",
        "    for j in wordCounterVal[i]:\n",
        "      likeliVal = (j + 1)/(sum(wordCounterVal[i]) + len(list_feature))\n",
        "      perClass[i].append(likeliVal)\n",
        "  return perClass"
      ],
      "metadata": {
        "id": "nJTgPyclFNdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naiveBayes(priorVal, likelihoodVal, tfTest):\n",
        "  collector = []\n",
        "  result = []\n",
        "\n",
        "  #Collect all likelihood result if the words are in test\n",
        "  for i in range(len(tfTest)):\n",
        "    collector.append([])\n",
        "    for j in range(len(tfTest[i])):\n",
        "        if tfTest[i][j]!=0:\n",
        "            collector[i].append(likelihoodVal[j])\n",
        "\n",
        "  for i in collector:\n",
        "    total = 1\n",
        "    for j in i:\n",
        "      total*=j\n",
        "    result.append(priorVal*total)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "bkkDa9E7Vh6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def voteClass(firstClassResult, secondClassResult, targetClass):\n",
        "  result = []\n",
        "  for i in range(len(firstClassResult)):\n",
        "    if firstClassResult[i]>secondClassResult[i]:\n",
        "      result.append(targetClass)\n",
        "    elif firstClassResult[i]<secondClassResult[i]:\n",
        "      result.append(\"NON \"+targetClass)\n",
        "  return result"
      ],
      "metadata": {
        "id": "8STI_qc6sbzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = '...'\n",
        "data_test = '...'\n",
        "\n",
        "#Parsing from xml\n",
        "list_ulasan, list_opini = parsing_xml(data_train)\n",
        "list_ulasan_test, list_opini_test = parsing_xml(data_test)"
      ],
      "metadata": {
        "id": "SJ4R0SBWy1xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing text\n",
        "list_ulasan_prepos = preprocessing(list_ulasan)\n",
        "list_ulasan_prepos_test = preprocessing(list_ulasan_test)"
      ],
      "metadata": {
        "id": "TeCbPQBbK0lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decontracted \n",
        "list_decontracted = decontracted(list_ulasan_prepos)\n",
        "list_decontracted_test = decontracted(list_ulasan_prepos_test)\n",
        "list_decontracted_copy = deepcopy(list_decontracted)\n",
        "list_decontracted_test_copy = deepcopy(list_decontracted_test)"
      ],
      "metadata": {
        "id": "tkW_QPFEK4S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Postag\n",
        "list_postag = postag(list_decontracted)\n",
        "list_postag_test = postag(list_decontracted_test)"
      ],
      "metadata": {
        "id": "LjOIPIokK75K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Opini Rule\n",
        "list_opini_rule = opini_rule(list_postag)\n",
        "list_opini_rule_uji = opini_rule(list_postag_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yVHED2sLFjA",
        "outputId": "612cbfe9-5b11-404a-d420-bd5e83e27f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: parsing empty text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Opinion Extractor\n",
        "list_extractor = opini_extractor(list_opini_rule)\n",
        "list_extractor_uji = opini_extractor(list_opini_rule_uji)"
      ],
      "metadata": {
        "id": "bqY3FIfxLHGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Extractor\n",
        "list_opini_new = class_extractor(list_decontracted_copy,list_extractor,list_opini)\n",
        "list_opini_test_new = class_extractor(list_decontracted_test_copy,list_extractor_uji,list_opini_test)"
      ],
      "metadata": {
        "id": "J29tXNGLhWq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Word as Feature\n",
        "list_feature = extractFeature(list_extractor)\n",
        "list_feature = extractFeature(list_extractor,list_feature)"
      ],
      "metadata": {
        "id": "tBxB5y3QLIgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF\n",
        "list_tf = termFrequency(list_extractor,list_feature)\n",
        "list_tf_uji = termFrequency(list_extractor_uji,list_feature)"
      ],
      "metadata": {
        "id": "HnMYKYzzLJ6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Flattening from 2D to 1D\n",
        "termFreq = list(chain.from_iterable(list_tf))\n",
        "\n",
        "termFreq_Test = list(chain.from_iterable(list_tf_uji))\n",
        "\n",
        "class_list = list(chain.from_iterable(list_opini_new))\n",
        "\n",
        "class_ListTest = list(chain.from_iterable(list_opini_test_new))"
      ],
      "metadata": {
        "id": "12_UPcdIFcOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deviding list with class sentiment and aspect into two variable\n",
        "aspectClassTest, sentimentClassTest = devideClass(class_ListTest)\n",
        "\n",
        "#Change 'other class' to 'Non ....' \n",
        "mainTargetClass = \"RESTAURANT#GENERAL\"\n",
        "aspectClassTestNew = [\"NON \"+mainTargetClass if x != mainTargetClass else mainTargetClass for x in aspectClassTest]\n",
        "sentimentClassTestNew = [\"NON \"+mainTargetClass if x != mainTargetClass else mainTargetClass for x in sentimentClassTest]"
      ],
      "metadata": {
        "id": "hq_t-8HPwsC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting prior\n",
        "tyu = prior(termFreq,class_list, mainTargetClass)"
      ],
      "metadata": {
        "id": "b4X8_k4HCT0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count total word per class\n",
        "tes = wordCounter(termFreq, class_list, mainTargetClass)"
      ],
      "metadata": {
        "id": "riWJ2ChmQ5Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count likelihood\n",
        "tesets = likelihood(tes,list_feature)"
      ],
      "metadata": {
        "id": "RXglYiAFVAvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count naive bayes both class \n",
        "#First class = main class\n",
        "#Second class = 'other' class\n",
        "firstClassResult = naiveBayes(tyu[0],tesets[0], termFreq_Test)\n",
        "secondClassResult = naiveBayes(tyu[1],tesets[1], termFreq_Test)"
      ],
      "metadata": {
        "id": "tKIMMQ9XnOBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vote from both output to determine data test class\n",
        "final = voteClass(firstClassResult,secondClassResult, mainTargetClass)"
      ],
      "metadata": {
        "id": "gMN3PiU-teFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the accuracy and other evaluation from the result\n",
        "print(f\"Accuracy : {accuracy_score(aspectClassTestNew, final) * 100} %\\n\") \n",
        "print(f\"Classification Report : \\n\\n{classification_report(aspectClassTestNew, final)}\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQU0cEfsvESj",
        "outputId": "c4e1cb0c-1f70-44a8-db2b-79f0182d67b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 84.83412322274881 %\n",
            "\n",
            "Classification Report : \n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "NON RESTAURANT#GENERAL       0.87      0.96      0.91       170\n",
            "    RESTAURANT#GENERAL       0.70      0.39      0.50        41\n",
            "\n",
            "              accuracy                           0.85       211\n",
            "             macro avg       0.78      0.67      0.71       211\n",
            "          weighted avg       0.83      0.85      0.83       211\n",
            "\n"
          ]
        }
      ]
    }
  ]
}